{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gacerioni/redis-workshop-json-search-vs/blob/master/redis_workshop_vector_intro_pt_br_gabs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRlp93K0C1q8"
      },
      "source": [
        "# Workshop - Redis como VectorDB - INTRO (TEM OUTROS!)\n",
        "\n",
        "## Vector Searches & Large Language Models\n",
        "\n",
        "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
        "\n",
        "\n",
        "Bem-vind[ao]s ao Workshop! Vamos ter uma experi√™ncia hands-on sobre alguns temas centrais do Redis, bem al√©m do Caching.\n",
        "\n",
        "\n",
        "Para uma experi√™ncia premium, como a que eu quero que voc√™s tenham, recomendo MUITO utilizar o Redis Insight (App ou Web) pra apoiar na visualiza√ß√£o dos dados.\n",
        "\n",
        "https://redis.com/redis-enterprise/redis-insight/\n",
        "\n",
        "---\n",
        "\n",
        "Novamente, vamos direto ao ponto. Para pegar o fio da meada, passando pela introdu√ß√£o, veja este outro notebook [aqui](https://colab.research.google.com/github/gacerioni/redis-workshop-json-search-vs/blob/master/redis-workshop-vector-similarity-search.ipynb).\n",
        "\n",
        "---\n",
        "\n",
        "## Objetivos do Workshop\n",
        "\n",
        "Este Notebook √© uma pequena demonstra√ß√£o do Redis como um Vector DB. Depois, vamos ver uma implementa√ß√£o de RAG e Semantic/LLM Caching.\n",
        "\n",
        "\n",
        "Espero que gostem! üññ\n",
        "\n",
        "\n",
        "## Conceito - Bancos de dados de vetores\n",
        "\n",
        "Os dados s√£o frequentemente n√£o estruturados, o que significa que n√£o s√£o descritos por um esquema bem definido. Exemplos de dados n√£o estruturados incluem trechos de texto, imagens, v√≠deos ou √°udio. Uma abordagem para armazenar e pesquisar dados n√£o estruturados √© usar embeddings de vetores.\n",
        "\n",
        "**O que s√£o vetores?**\\\n",
        "Em aprendizado de m√°quina e IA, vetores s√£o sequ√™ncias de n√∫meros que representam dados. Eles s√£o as entradas e sa√≠das dos modelos, encapsulando informa√ß√µes subjacentes em uma forma num√©rica. Vetores transformam dados n√£o estruturados, como textos, imagens, v√≠deos e √°udios, em um formato que os modelos de aprendizado de m√°quina podem processar.\n",
        "\n",
        "**Por que eles s√£o importantes?**\\\n",
        "Vetores capturam padr√µes complexos e significados sem√¢nticos inerentes aos dados, tornando-os ferramentas poderosas para uma variedade de aplica√ß√µes. Eles permitem que modelos de aprendizado de m√°quina compreendam e manipulem dados n√£o estruturados de forma mais eficaz.\n",
        "\n",
        "**Melhorando a busca tradicional.**\\\n",
        "A busca tradicional por palavras-chave ou lexical depende de correspond√™ncias exatas de palavras ou frases, o que pode ser limitante. Em contraste, a busca vetorial, ou busca sem√¢ntica, aproveita a rica informa√ß√£o capturada nos embeddings de vetores. Ao mapear dados em um espa√ßo vetorial, itens semelhantes s√£o posicionados pr√≥ximos uns dos outros com base em seu significado. Essa abordagem permite resultados de busca mais precisos e significativos, pois considera o contexto e o conte√∫do sem√¢ntico da consulta, e n√£o apenas as palavras exatas usadas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 1 - Criar uma conta Free no Redis Cloud\n",
        "\n",
        "Basta seguir o passo a passo [aqui](https://colab.research.google.com/github/gacerioni/redis-workshop-notebook-validator/blob/master/redis-workshop-setup-notebook-validator.ipynb)!"
      ],
      "metadata": {
        "id": "EAWGcj0DqKBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup R√°pido\n",
        "\n",
        "## Instala√ßao das libs do Python e redis-cli"
      ],
      "metadata": {
        "id": "tW9m9pffaMl-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-9YfABV3_1z"
      },
      "outputs": [],
      "source": [
        "# Instale o Redis client e tambem o Hugging Face sentence transformers, pois vamos gerar os vetores aqui mesmo\n",
        "!pip install -q redis sentence_transformers\n",
        "\n",
        "# Instale tbm algumas libs pra gente brincar com LLM\n",
        "# este comando baixa umas paradas de 700MB+, ok? Leva dois minutinhos.\n",
        "!pip install -r https://raw.githubusercontent.com/gacerioni/redis-workshop-json-search-vs/master/deps/llm-movies/requirements.txt\n",
        "\n",
        "# E instalar a CLI, via redis-tools, que inclui a famosa redis-cli\n",
        "!apt-get update\n",
        "!apt-get install -y redis-tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deployment do Redis Stack - Um passo importante para esta demo em espec√≠fico\n",
        "\n",
        "Pessoal, esse Workshop de VectorSearch vai passar de 30MB.\n",
        "\n",
        "---\n",
        "\n",
        "**Ou seja: pra gente poder fazer esse e os futuros de Vector com LLM, vamos rodar o `redis-stack` aqui, locamente, neste notebook mesmo.**\n",
        "\n",
        "---\n",
        "\n",
        "Para isso, basta executar:"
      ],
      "metadata": {
        "id": "qB7ouggQvSwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
        "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
        "sudo apt-get update  > /dev/null 2>&1\n",
        "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "redis-stack-server --daemonize yes"
      ],
      "metadata": {
        "id": "5IErGQBkv18f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhS6htjOCxr_"
      },
      "source": [
        "### Conectando com o Redis server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UxA0cSkBYgo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Coloque aqui os dados do seu DB do Redis Cloud\n",
        "REDIS_HOST=\"localhost\"\n",
        "REDIS_PORT=6379\n",
        "REDIS_PASSWORD=\"\"\n",
        "\n",
        "# Caso o SSL esteja ativo pro endpoint, adicione --tls\n",
        "# Recomendo n√£o misturar l√© com cr√© aqui, visto que n√£o vamos ter nenhuma informa√ß√£o sens√≠vel passando pelo fio.\n",
        "if REDIS_PASSWORD!=\"\":\n",
        "  os.environ[\"REDIS_CONN\"]=f\"-h {REDIS_HOST} -p {REDIS_PORT} -a {REDIS_PASSWORD} --no-auth-warning\"\n",
        "else:\n",
        "  os.environ[\"REDIS_CONN\"]=f\"-h {REDIS_HOST} -p {REDIS_PORT}\"\n",
        "\n",
        "# Caso o SSL esteja ativo pro endpoint, use rediss:// como o URL prefix\n",
        "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\"\n",
        "INDEX_NAME = f\"qna:idx\"\n",
        "\n",
        "# Teste a Redis connection\n",
        "!redis-cli $REDIS_CONN PING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEIgz0Lvg9Im"
      },
      "outputs": [],
      "source": [
        "# Testando via Python (redis-py)\n",
        "import redis\n",
        "redis = redis.Redis(\n",
        "  host=REDIS_HOST,\n",
        "  port=REDIS_PORT,\n",
        "  password=REDIS_PASSWORD)\n",
        "redis.ping()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importando e preparando as libs que iremos usar\n",
        "\n",
        "Este primeiro bloco vai garantir que todas as depend√™ncias estejam prontas pra gente brincar com o ChatGPT usando o Redis como um c√©rebro extra e atualizado.\n",
        "\n",
        "O bloco de vari√°veis ali √© basicamente eu explicando algumas prefer√™ncias minhas pro Vector Search, como quantas dimens√µes meus vetores possuem. 384, neste caso."
      ],
      "metadata": {
        "id": "NBVqcG1rhkU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsJLTPAb36Z3"
      },
      "outputs": [],
      "source": [
        "import redis\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "from sentence_transformers import *\n",
        "from redis.commands.search.query import Query\n",
        "from redis.commands.search.field import TextField, TagField, VectorField\n",
        "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
        "import openai\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "redis = redis.Redis(\n",
        "  host=REDIS_HOST,\n",
        "  port=REDIS_PORT,\n",
        "  password=REDIS_PASSWORD,\n",
        "  decode_responses=True)\n",
        "\n",
        "# Preferencias acerca do nosso Vector Search e como o Redis deve indexar os embeddings\n",
        "VSS_INDEX_TYPE = \"HNSW\"\n",
        "VSS_DATA_TYPE = \"FLOAT32\"\n",
        "VSS_DISTANCE = \"COSINE\"\n",
        "VSS_DIMENSION = 384\n",
        "VSS_MINIMUM_SCORE = 2\n",
        "\n",
        "MAX_MOVIES = 5000000\n",
        "\n",
        "\n",
        "redis.ping()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iniciando os trabalhos\n",
        "\n",
        "## Passo 1 - Carregando os filmes no Redis\n",
        "\n",
        "Primeiro, vamos baixar o CSV do nosso GitHub, contendo um monte de filmes.\\\n",
        "Eu fiz um dump da base do **[IMDB](https://www.imdb.com/)**, caso estejam curiosos.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "92yNW9uNbbI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixando\n",
        "!wget https://raw.githubusercontent.com/gacerioni/gabs-chatbot-llm-gpt-redis-vector-rag-demo/main/data/movies/imdb_movies.csv"
      ],
      "metadata": {
        "id": "gqjlZ2q1xQrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "    with open(\"imdb_movies.csv\", encoding='utf-8') as csvf:\n",
        "        csvReader = csv.DictReader(csvf)\n",
        "        cnt = 0\n",
        "        for row in csvReader:\n",
        "            redis.json().set(f'moviebot:movie:{cnt}', '$', row)\n",
        "            cnt = cnt + 1\n",
        "            if (cnt > MAX_MOVIES):\n",
        "                break\n",
        "        print(\"Data was loaded into Redis!\")\n",
        "\n",
        "# Flush no DB, pra gente come√ßar do 0\n",
        "redis.flushdb()\n",
        "\n",
        "# Carregar os dados no Redis\n",
        "load()\n",
        "\n",
        "\n",
        "# Contar as chaves no Redis, s√≥ pra ver se tudo foi carregado\n",
        "key_count = redis.dbsize()\n",
        "print(f\"Total de chaves no Redis: {key_count}\")"
      ],
      "metadata": {
        "id": "7oKdWHAnrNvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E ver alguns dados, s√≥ pra ter certeza que subiram para o Redis.\n",
        "\n",
        "Notem que a gente n√£o criou os vetores ainda, bel√™?"
      ],
      "metadata": {
        "id": "Oh_eguGCy7OS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "redis.json().get(\"moviebot:movie:1\")"
      ],
      "metadata": {
        "id": "LeAbHKtQzAXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wz8e4UMox-vC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz5QOrxWCFd8"
      },
      "source": [
        "## Passo 2 - Embedding - Hora de gerar vetores!\n",
        "\n",
        "Finalmente, vamos ver nosso banco de vetores tomando forma.\n",
        "\n",
        "Para isso, pegaremos alguns campos de informa√ß√µes n√£o estruturadas de cada filme. Vamos concatenar dados interessantes como t√≠tulo, g√™nero, equipe, pontua√ß√£o e sinopse.\n",
        "\n",
        "Com essa string concatenada, geraremos vetores que representam cada filme de maneira embutida. Esses vetores ser√£o armazenados no Redis, permitindo buscas e an√°lises sem√¢nticas avan√ßadas sobre os filmes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEXNz8-TCEfA"
      },
      "outputs": [],
      "source": [
        "# esse comando pode levar 1-2 minutos, ok?\n",
        "def create_embeddings():\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    for key in redis.scan_iter(match='moviebot:movie:*'):\n",
        "        print(f\"creating the embedding for {key}\")\n",
        "        result = redis.json().get(key, \"$.names\", \"$.overview\", \"$.crew\", \"$.score\", \"$.genre\")\n",
        "        movie = f\"movie title is: {result['$.names'][0]}\\n\"\n",
        "        movie += f\"movie genre is: {result['$.genre'][0]}\\n\"\n",
        "        movie += f\"movie crew is: {result['$.crew'][0]}\\n\"\n",
        "        movie += f\"movie score is: {result['$.score'][0]}\\n\"\n",
        "        movie += f\"movie overview is: {result['$.overview'][0]}\\n\"\n",
        "        redis.json().set(key, \"$.overview_embedding\", model.encode(movie).astype(np.float32).tolist())\n",
        "\n",
        "create_embeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checkpoint - vamos entender o que temos no Redis agora\n",
        "\n",
        "N√≥s criamos o embedding usando outros atributos do filme, e guardamos esse \"fingerprint\" junto do pr√≥prio documento JSON que representa cada filme.\n",
        "\n",
        "Olha s√≥ como ficou agora:"
      ],
      "metadata": {
        "id": "fP_KOaSk5t-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "movie_with_embedding = redis.json().get(\"moviebot:movie:1\")\n",
        "pretty_json = json.dumps(movie_with_embedding, indent=4, ensure_ascii=False)\n",
        "print(pretty_json)"
      ],
      "metadata": {
        "id": "CQ-nncTZ6DcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 3 - Criando o √≠ndex para o JSON do Filme\n",
        "\n",
        "Vamos criar o √≠ndex, sem segredo, pra gente fazer as queries depois."
      ],
      "metadata": {
        "id": "iJpmj0Tf6-sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def create_index():\n",
        "    indexes = redis.execute_command(\"FT._LIST\")\n",
        "    if \"movie_idx\" not in indexes:\n",
        "        index_def = IndexDefinition(prefix=[\"moviebot:movie:\"], index_type=IndexType.JSON)\n",
        "        schema = (TextField(\"$.crew\", as_name=\"crew\"),\n",
        "                  TextField(\"$.overview\", as_name=\"overview\"),\n",
        "                  TagField(\"$.genre\", as_name=\"genre\"),\n",
        "                  TagField(\"$.names\", as_name=\"names\"),\n",
        "                  VectorField(\"$.overview_embedding\", VSS_INDEX_TYPE,\n",
        "                              {\"TYPE\": VSS_DATA_TYPE, \"DIM\": VSS_DIMENSION, \"DISTANCE_METRIC\": VSS_DISTANCE},\n",
        "                              as_name=\"embedding\"))\n",
        "        redis.ft('movie_idx').create_index(schema, definition=index_def)\n",
        "        print(\"The index has been created\")\n",
        "    else:\n",
        "        print(\"The index exists\")\n",
        "\n",
        "# Cria o index aqui mesmo\n",
        "create_index()"
      ],
      "metadata": {
        "id": "uY-4e17V7R-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E vamos testar o index por aqui mesmo tamb√©m, fazendo uma Full Text Search bem pregui√ßosa"
      ],
      "metadata": {
        "id": "S-Bi5RqQ77MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testa aqui mesmo\n",
        "time.sleep(2)\n",
        "\n",
        "# Executar a busca, direto aqui mesmo\n",
        "# Redis √© binary safe, ent√£o esse abaixo, com √±, precisa funfar de boa\n",
        "search_result = redis.execute_command(\"FT.SEARCH\", \"movie_idx\", \"@crew:'Zoe Salda√±a'\")\n",
        "print(search_result)"
      ],
      "metadata": {
        "id": "eYldX9M38AXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 4 - Criando as fun√ß√µes Python de apoio ao ChatGPT LLM\n",
        "\n",
        "Vamos usar este pequeno bloco de c√≥digo para configurar alguns detalhes para fazer funcionar toda a nossa integra√ß√£o.\n",
        "\n",
        "---\n",
        "\n",
        "### O que deve acontecer agora?\n",
        "\n",
        "**Estas duas fun√ß√µes abaixo s√£o para orquestrar a comunica√ß√£o entre o usu√°rio/cliente e o chatgpt.**\n",
        "\n",
        "Isso por si s√≥ j√° √© um RAG! Em detalhes, √© isso que ocorre:\n",
        "\n",
        "\n",
        "\n",
        "*   Pedimos para o usu√°rio fazer uma pergunta em linguagem natural. Neste caso, buscando por filmes.\n",
        "*   Transformamos a busca do cliente em um vetor usando o mesmo modelo utilizado para criar os embeddings.\n",
        "*   Realizamos a busca no Redis usando o vetor. Um Vector Search por dist√¢ncia de cosseno, pois funciona bem pro nosso caso de uso.\n",
        "*   Pegamos os filmes encontrados, e vamos us√°-los como contexto pro ChatGPT ficar mais ligeiro sobre o assunto que estamos falando. √â aqui que a gente come√ßa a diminuir as temidas alucina√ß√µes do GPT4!\n",
        "*   Preparamos uma mensagem sist√™mica para definir o tom do LLM e especificar o que esperamos dele. Enviamos em texto pro chatgpt, dizendo o que esperamos dele.\n",
        "*   Geramos a intera√ß√£o, guardamos a resposta, e entregamos ao cliente."
      ],
      "metadata": {
        "id": "QmQSDzvpEOM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VSS_LLM_RAG_MODEL = \"gpt-3.5-turbo-0613\"\n",
        "OPENAI_API_KEY = \"nada\"\n",
        "VSS_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "def get_prompt(model, query):\n",
        "    context = \"\"\n",
        "    prompt = \"\"\n",
        "\n",
        "    # Configura a query de busca vetorial no Redis\n",
        "    q = Query(\"@embedding:[VECTOR_RANGE $radius $vec]=>{$YIELD_DISTANCE_AS: score}\") \\\n",
        "        .sort_by(\"score\", asc=True) \\\n",
        "        .return_fields(\"overview\", \"names\", \"score\", \"$.crew\", \"$.genre\", \"$.score\") \\\n",
        "        .paging(0, 5) \\\n",
        "        .dialect(2)\n",
        "\n",
        "    # Define os par√¢metros da query\n",
        "    query_params = {\n",
        "        \"radius\": VSS_MINIMUM_SCORE,\n",
        "        \"vec\": model.encode(query).astype(np.float32).tobytes()\n",
        "    }\n",
        "\n",
        "    # Executa a busca no Redis\n",
        "    res = redis.ft(\"movie_idx\").search(q, query_params)\n",
        "\n",
        "    # Processa os resultados da busca\n",
        "    if (res is not None) and len(res.docs):\n",
        "        it = iter(res.docs[0:])\n",
        "        for x in it:\n",
        "            movie = f\"movie title is: {x['names']}\\n\"\n",
        "            movie += f\"movie genre is: {x['$.genre']}\\n\"\n",
        "            movie += f\"movie crew is: {x['$.crew']}\\n\"\n",
        "            movie += f\"movie score is: {x['$.score']}\\n\"\n",
        "            movie += f\"movie overview is: {x['overview']}\\n\"\n",
        "            context += movie + \"\\n\"\n",
        "\n",
        "    # Cria o prompt para o LLM\n",
        "    if len(context) > 0:\n",
        "        prompt = '''Use the provided information to answer the search query the user has sent. The information in the\n",
        "        database provides three movies, choose the one or the ones that fit most. If you can't answer the user's\n",
        "        question, say \"Sorry, I am unable to answer the question, try to refine your question\". Do not guess. You\n",
        "        must deduce the answer exclusively from the information provided. The answer must be formatted in markdown or\n",
        "        HTML. Do not make things up. Do not add personal opinions. Do not add any disclaimer.\n",
        "\n",
        "            Search query:\n",
        "\n",
        "            {}\n",
        "\n",
        "            Information in the database:\n",
        "\n",
        "            {}\n",
        "            '''.format(query, context)\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def getOpenAIGPT35(prompt):\n",
        "    # Define a mensagem do sistema\n",
        "    system_msg = ('You are a smart and knowledgeable AI assistant with expertise in all kinds of movies. You are a '\n",
        "                  'very friendly and helpful AI. You are empowered to recommend movies based on the provided context. '\n",
        "                  'Do NOT make anything up. Do NOT engage in topics that are not about movies.')\n",
        "\n",
        "    # Define a codifica√ß√£o\n",
        "    encoding = tiktoken.encoding_for_model(VSS_LLM_RAG_MODEL)\n",
        "\n",
        "    try:\n",
        "        openai.api_key = OPENAI_API_KEY\n",
        "        response = openai.ChatCompletion.create(model=VSS_LLM_RAG_MODEL,\n",
        "                                                stream=False,\n",
        "                                                messages=[{\"role\": \"system\", \"content\": system_msg},\n",
        "                                                          {\"role\": \"user\", \"content\": prompt}])\n",
        "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except openai.error.OpenAIError as e:\n",
        "        # Trata erros\n",
        "        if \"context window is too large\" in str(e):\n",
        "            print(\"Error: Maximum context length exceeded. Please shorten your input.\")\n",
        "            return \"Maximum context length exceeded\"\n",
        "        else:\n",
        "            print(\"An unexpected error occurred:\", e)\n",
        "            return \"An unexpected error occurred\""
      ],
      "metadata": {
        "id": "a-8_BNFkJPjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 5 - Hora de testar pra valer!\n",
        "\n",
        "**Parab√©ns por ter chegado at√© aqui!**\n",
        "\n",
        "Vamos fazer algo bem simples, pra fechar com chave de ouro.\n",
        "\n",
        "Vamos criar um loop pra fazer essa ponte entre a nossa APP e o ChatGPT.\\\n",
        "Isso √© literalmente o que vemos em produ√ß√£o, nos clientes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l59yO6HNKOLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def render():\n",
        "    model = SentenceTransformer(VSS_MODEL)\n",
        "    # Reage √† entrada do usu√°rio\n",
        "    while True:\n",
        "        question = input(\"Me pe√ßa indica√ß√µes sobre filmes:\\n\")  # Pede para o usu√°rio fazer uma pergunta\n",
        "        reply = f\"Sua pergunta foi: {question}\"\n",
        "        prompt = get_prompt(model, question)  # Gera o prompt a partir da pergunta do usu√°rio\n",
        "        response = getOpenAIGPT35(prompt)  # Obt√©m a resposta do LLM\n",
        "        print(response)  # Exibe a resposta\n",
        "        print(\"--------------------------------\")\n",
        "\n",
        "# Chama o loop por tempo indeterminado\n",
        "render()"
      ],
      "metadata": {
        "id": "rZFuEh55Oj6G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}